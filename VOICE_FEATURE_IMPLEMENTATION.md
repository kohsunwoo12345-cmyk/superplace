# ğŸ¤ ìŒì„± ê¸°ëŠ¥ êµ¬í˜„ ì™„ë£Œ ê°€ì´ë“œ

**ì‘ì„±ì¼**: 2026-01-24  
**ìƒíƒœ**: âœ… ì™„ë£Œ

## ğŸ“‹ ê°œìš”

ì‚¬ìš©ìê°€ **ìŒì„±ì„ ë…¹ìŒí•˜ì—¬ ë©”ì‹œì§€ë¥¼ ì „ì†¡**í•˜ê³ , **ë´‡ì˜ ì‘ë‹µì„ ìŒì„±ìœ¼ë¡œ ë“£ëŠ”** ê¸°ëŠ¥ì„ ì™„ì „íˆ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.

## âœ¨ êµ¬í˜„ëœ ê¸°ëŠ¥

### 1ï¸âƒ£ ì‚¬ìš©ì ìŒì„± ì…ë ¥ (STT - Speech to Text)
- **ë…¹ìŒ ì‹œì‘/ì¤‘ì§€**: ë§ˆì´í¬ ë²„íŠ¼ í´ë¦­ìœ¼ë¡œ ë…¹ìŒ ì‹œì‘/ì¤‘ì§€
- **ìë™ í…ìŠ¤íŠ¸ ë³€í™˜**: ë…¹ìŒ ì™„ë£Œ ì‹œ ìë™ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ë˜ì–´ ì…ë ¥ì°½ì— í‘œì‹œ
- **OpenAI Whisper API ì‚¬ìš©**: `/api/speech/transcribe` ì—”ë“œí¬ì¸íŠ¸
- **ì˜¤ë””ì˜¤ í˜•ì‹**: WebM (ë¸Œë¼ìš°ì € ì§€ì›)

### 2ï¸âƒ£ ë´‡ ì‘ë‹µ ìŒì„± ì¶œë ¥ (TTS - Text to Speech)
- **ìë™ ìŒì„± ì¬ìƒ**: ë´‡ ì‘ë‹µ í›„ 0.3ì´ˆ ë’¤ ìë™ìœ¼ë¡œ ìŒì„± ì¬ìƒ (enableVoiceOutputì´ trueì¼ ë•Œ)
- **ìˆ˜ë™ ìŒì„± ì¬ìƒ**: ê° ë©”ì‹œì§€ í•˜ë‹¨ì˜ "ìŒì„± ë“£ê¸°" ë²„íŠ¼ìœ¼ë¡œ ìˆ˜ë™ ì¬ìƒ ê°€ëŠ¥
- **ì¬ìƒ ì¤‘ì§€**: ì¬ìƒ ì¤‘ "ì¤‘ì§€" ë²„íŠ¼ìœ¼ë¡œ ì–¸ì œë“  ì¤‘ë‹¨ ê°€ëŠ¥
- **OpenAI TTS API ì‚¬ìš©**: `/api/speech/synthesize` ì—”ë“œí¬ì¸íŠ¸
- **ì˜¤ë””ì˜¤ í˜•ì‹**: MP3 (audio/mpeg)

## ğŸ›  êµ¬í˜„ ì„¸ë¶€ì‚¬í•­

### í´ë¼ì´ì–¸íŠ¸ ì¸¡ (`src/app/dashboard/ai-gems/[gemId]/page.tsx`)

#### ìƒíƒœ ê´€ë¦¬
```typescript
const [isRecording, setIsRecording] = useState(false);
const [audioBlob, setAudioBlob] = useState<Blob | null>(null);
const [mediaRecorder, setMediaRecorder] = useState<MediaRecorder | null>(null);
const [isPlayingAudio, setIsPlayingAudio] = useState<string | null>(null);
const audioRef = useRef<HTMLAudioElement | null>(null);
```

#### ìŒì„± ë…¹ìŒ í•¨ìˆ˜
```typescript
const startRecording = async () => {
  try {
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    const recorder = new MediaRecorder(stream);
    const chunks: BlobPart[] = [];

    recorder.ondataavailable = (e) => {
      if (e.data.size > 0) {
        chunks.push(e.data);
      }
    };

    recorder.onstop = async () => {
      const blob = new Blob(chunks, { type: 'audio/webm' });
      setAudioBlob(blob);
      stream.getTracks().forEach(track => track.stop());
      await transcribeAudio(blob);
    };

    recorder.start();
    setMediaRecorder(recorder);
    setIsRecording(true);
  } catch (error) {
    console.error('ë…¹ìŒ ì˜¤ë¥˜:', error);
    alert('ë§ˆì´í¬ ì ‘ê·¼ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤.');
  }
};

const stopRecording = () => {
  if (mediaRecorder && mediaRecorder.state !== 'inactive') {
    mediaRecorder.stop();
    setIsRecording(false);
  }
};
```

#### ìŒì„±â†’í…ìŠ¤íŠ¸ ë³€í™˜ í•¨ìˆ˜
```typescript
const transcribeAudio = async (audioBlob: Blob) => {
  try {
    const formData = new FormData();
    formData.append('audio', audioBlob, 'recording.webm');

    const response = await fetch('/api/speech/transcribe', {
      method: 'POST',
      body: formData,
    });

    if (!response.ok) {
      throw new Error('ìŒì„± ë³€í™˜ ì‹¤íŒ¨');
    }

    const data = await response.json();
    setInput(data.text); // ë³€í™˜ëœ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ì°½ì— ì„¤ì •
  } catch (error) {
    console.error('ìŒì„± ë³€í™˜ ì˜¤ë¥˜:', error);
    alert('ìŒì„± ë³€í™˜ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.');
  } finally {
    setAudioBlob(null);
  }
};
```

#### í…ìŠ¤íŠ¸â†’ìŒì„± ì¬ìƒ í•¨ìˆ˜
```typescript
const playTextAsAudio = async (text: string, messageId: string) => {
  try {
    setIsPlayingAudio(messageId);

    const response = await fetch('/api/speech/synthesize', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({ text }),
    });

    if (!response.ok) {
      throw new Error('ìŒì„± ìƒì„± ì‹¤íŒ¨');
    }

    const audioBlob = await response.blob();
    const audioUrl = URL.createObjectURL(audioBlob);

    if (audioRef.current) {
      audioRef.current.pause();
    }

    const audio = new Audio(audioUrl);
    audioRef.current = audio;

    audio.onended = () => {
      setIsPlayingAudio(null);
      URL.revokeObjectURL(audioUrl);
    };

    audio.onerror = () => {
      setIsPlayingAudio(null);
      alert('ìŒì„± ì¬ìƒì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.');
    };

    await audio.play();
  } catch (error) {
    console.error('ìŒì„± ì¬ìƒ ì˜¤ë¥˜:', error);
    setIsPlayingAudio(null);
    alert('ìŒì„± ì¬ìƒì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.');
  }
};
```

#### ìë™ ìŒì„± ì¬ìƒ (ë´‡ ì‘ë‹µ í›„)
```typescript
const assistantMessage: Message = {
  role: 'assistant',
  content: data.response,
  timestamp: new Date(),
};

setMessages((prev) => [...prev, assistantMessage]);

// ìŒì„± ì¶œë ¥ì´ í™œì„±í™”ë˜ì–´ ìˆìœ¼ë©´ ìë™ ì¬ìƒ
if (gem.enableVoiceOutput) {
  setTimeout(() => {
    playTextAsAudio(data.response, `${messages.length + 1}`);
  }, 300);
}
```

### ì„œë²„ ì¸¡ API

#### `/api/speech/transcribe` (STT)
```typescript
// src/app/api/speech/transcribe/route.ts
import { NextRequest, NextResponse } from "next/server";
import { getServerSession } from "next-auth";
import { authOptions } from "@/lib/auth";

export async function POST(req: NextRequest) {
  const session = await getServerSession(authOptions);
  if (!session?.user) {
    return NextResponse.json({ error: "ì¸ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤" }, { status: 401 });
  }

  try {
    const formData = await req.formData();
    const audioFile = formData.get('audio') as File;

    if (!audioFile) {
      return NextResponse.json({ error: "ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤" }, { status: 400 });
    }

    // OpenAI Whisper API í˜¸ì¶œ
    const openaiFormData = new FormData();
    openaiFormData.append('file', audioFile);
    openaiFormData.append('model', 'whisper-1');
    openaiFormData.append('language', 'ko');

    const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${process.env.OPENAI_API_KEY}`,
      },
      body: openaiFormData,
    });

    if (!response.ok) {
      const errorData = await response.text();
      console.error('OpenAI Whisper API ì˜¤ë¥˜:', errorData);
      return NextResponse.json(
        { error: "ìŒì„± ë³€í™˜ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤" },
        { status: 500 }
      );
    }

    const data = await response.json();
    return NextResponse.json({ text: data.text });
  } catch (error) {
    console.error('ìŒì„± ë³€í™˜ ì˜¤ë¥˜:', error);
    return NextResponse.json(
      { error: "ìŒì„± ë³€í™˜ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤" },
      { status: 500 }
    );
  }
}
```

#### `/api/speech/synthesize` (TTS)
```typescript
// src/app/api/speech/synthesize/route.ts
import { NextRequest, NextResponse } from "next/server";
import { getServerSession } from "next-auth";
import { authOptions } from "@/lib/auth";

export async function POST(req: NextRequest) {
  const session = await getServerSession(authOptions);
  if (!session?.user) {
    return NextResponse.json({ error: "ì¸ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤" }, { status: 401 });
  }

  try {
    const { text } = await req.json();

    if (!text || typeof text !== 'string') {
      return NextResponse.json({ error: "í…ìŠ¤íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤" }, { status: 400 });
    }

    // OpenAI TTS API í˜¸ì¶œ
    const response = await fetch('https://api.openai.com/v1/audio/speech', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${process.env.OPENAI_API_KEY}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'tts-1',
        voice: 'nova', // alloy, echo, fable, onyx, nova, shimmer
        input: text.slice(0, 4096), // ìµœëŒ€ 4096ì
        speed: 1.0,
      }),
    });

    if (!response.ok) {
      const errorData = await response.text();
      console.error('OpenAI TTS API ì˜¤ë¥˜:', errorData);
      return NextResponse.json(
        { error: "ìŒì„± ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤" },
        { status: 500 }
      );
    }

    const audioBuffer = await response.arrayBuffer();
    
    return new NextResponse(audioBuffer, {
      status: 200,
      headers: {
        'Content-Type': 'audio/mpeg',
        'Content-Length': audioBuffer.byteLength.toString(),
      },
    });
  } catch (error) {
    console.error('ìŒì„± ìƒì„± ì˜¤ë¥˜:', error);
    return NextResponse.json(
      { error: "ìŒì„± ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤" },
      { status: 500 }
    );
  }
}
```

## ğŸ¨ UI/UX êµ¬í˜„

### ìŒì„± ë…¹ìŒ ë²„íŠ¼
```tsx
{gem.enableVoiceInput && (
  <Button
    type="button"
    variant="outline"
    size="icon"
    onClick={isRecording ? stopRecording : startRecording}
    disabled={isLoading}
    className={`flex-shrink-0 transition-colors ${
      isRecording 
        ? 'bg-red-50 border-red-300 hover:bg-red-100' 
        : 'hover:bg-purple-50 hover:border-purple-300'
    }`}
  >
    {isRecording ? (
      <MicOff className="h-5 w-5 text-red-600 animate-pulse" />
    ) : (
      <Mic className="h-5 w-5" />
    )}
  </Button>
)}
```

### ìŒì„± ë“£ê¸° ë²„íŠ¼
```tsx
{message.role === 'assistant' && gem.enableVoiceOutput && (
  <div className="mt-2 flex justify-end">
    <Button
      type="button"
      variant="ghost"
      size="sm"
      onClick={() => {
        const messageId = `${index}`;
        if (isPlayingAudio === messageId) {
          stopAudio();
        } else {
          playTextAsAudio(message.content, messageId);
        }
      }}
      className="flex items-center gap-1 text-xs"
    >
      {isPlayingAudio === `${index}` ? (
        <>
          <VolumeX className="h-3 w-3" />
          <span>ì¤‘ì§€</span>
        </>
      ) : (
        <>
          <Volume2 className="h-3 w-3" />
          <span>ìŒì„± ë“£ê¸°</span>
        </>
      )}
    </Button>
  </div>
)}
```

## âš™ï¸ í™˜ê²½ ì„¤ì •

### í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜

**Vercel ëŒ€ì‹œë³´ë“œ**ì—ì„œ ë‹¤ìŒ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤:

```env
OPENAI_API_KEY=sk-proj-...your-api-key...
```

### Vercel í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ë°©ë²•

1. **Vercel ëŒ€ì‹œë³´ë“œ** ì ‘ì†: https://vercel.com/dashboard
2. í”„ë¡œì íŠ¸ ì„ íƒ: `superplace-study`
3. **Settings** íƒ­ í´ë¦­
4. ì¢Œì¸¡ ë©”ë‰´ì—ì„œ **Environment Variables** ì„ íƒ
5. ë‹¤ìŒ ë³€ìˆ˜ ì¶”ê°€:
   - **Key**: `OPENAI_API_KEY`
   - **Value**: `sk-proj-...` (OpenAI API í‚¤)
   - **Environment**: Production, Preview, Development ëª¨ë‘ ì„ íƒ
6. **Save** í´ë¦­
7. **Redeploy** í•„ìš”: Settings â†’ Deployments â†’ ìµœì‹  ë°°í¬ â†’ "..." â†’ Redeploy

## ğŸ§ª í…ŒìŠ¤íŠ¸ ë°©ë²•

### 1. ìŒì„± ì…ë ¥ í…ŒìŠ¤íŠ¸
1. ê´€ë¦¬ì ê³„ì •ìœ¼ë¡œ ë¡œê·¸ì¸
2. `/dashboard/admin/ai-bots-management`ì—ì„œ ìƒˆ ë´‡ ìƒì„±
3. **"ìŒì„± ì…ë ¥ í™œì„±í™”"** ì˜µì…˜ ì²´í¬
4. ë´‡ ì €ì¥ í›„ ì±„íŒ… í˜ì´ì§€ ì ‘ì†
5. **ë§ˆì´í¬ ë²„íŠ¼** í´ë¦­ â†’ ë…¹ìŒ ì‹œì‘ (ë¹¨ê°„ìƒ‰ ì ë©¸ í‘œì‹œ)
6. ìŒì„± ì…ë ¥ â†’ ë‹¤ì‹œ ë§ˆì´í¬ ë²„íŠ¼ í´ë¦­ â†’ ë…¹ìŒ ì¤‘ì§€
7. ìë™ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë³€í™˜ë˜ì–´ ì…ë ¥ì°½ì— í‘œì‹œ í™•ì¸
8. Enterë¡œ ë©”ì‹œì§€ ì „ì†¡

### 2. ìŒì„± ì¶œë ¥ í…ŒìŠ¤íŠ¸
1. ê´€ë¦¬ì ê³„ì •ìœ¼ë¡œ ë¡œê·¸ì¸
2. `/dashboard/admin/ai-bots-management`ì—ì„œ ìƒˆ ë´‡ ìƒì„±
3. **"ìŒì„± ì¶œë ¥ í™œì„±í™”"** ì˜µì…˜ ì²´í¬
4. ë´‡ ì €ì¥ í›„ ì±„íŒ… í˜ì´ì§€ ì ‘ì†
5. í…ìŠ¤íŠ¸ ë©”ì‹œì§€ ì „ì†¡
6. ë´‡ ì‘ë‹µ í›„ **ìë™ìœ¼ë¡œ ìŒì„± ì¬ìƒ** í™•ì¸
7. ë˜ëŠ” **"ìŒì„± ë“£ê¸°"** ë²„íŠ¼ í´ë¦­ìœ¼ë¡œ ìˆ˜ë™ ì¬ìƒ í…ŒìŠ¤íŠ¸
8. **"ì¤‘ì§€"** ë²„íŠ¼ìœ¼ë¡œ ì¬ìƒ ì¤‘ë‹¨ í…ŒìŠ¤íŠ¸

### 3. ë¸Œë¼ìš°ì € ê¶Œí•œ í™•ì¸
- Chrome/Edge: ì£¼ì†Œì°½ ì¢Œì¸¡ ğŸ”’ ì•„ì´ì½˜ â†’ ë§ˆì´í¬ ê¶Œí•œ "í—ˆìš©" í™•ì¸
- Firefox: ì£¼ì†Œì°½ ì¢Œì¸¡ ğŸ”’ ì•„ì´ì½˜ â†’ ê¶Œí•œ â†’ ë§ˆì´í¬ "í—ˆìš©" í™•ì¸
- Safari: Safari ë©”ë‰´ â†’ ì„¤ì • â†’ ì›¹ì‚¬ì´íŠ¸ â†’ ë§ˆì´í¬ â†’ í•´ë‹¹ ì‚¬ì´íŠ¸ "í—ˆìš©"

## ğŸš€ ë°°í¬ ìƒíƒœ

- **ì»¤ë°‹ ID**: (ìë™ ìƒì„± ì˜ˆì •)
- **ë¸Œëœì¹˜**: `main`
- **ë°°í¬ URL**: https://superplace-study.vercel.app
- **ë°°í¬ ìƒíƒœ**: Vercel ìë™ ë°°í¬ ì§„í–‰ ì¤‘ (ì•½ 2-3ë¶„ ì†Œìš”)

## ğŸ“Š ê¸°ëŠ¥ ìš”ì•½

| ê¸°ëŠ¥ | ìƒíƒœ | ì„¤ëª… |
|------|------|------|
| ìŒì„± ë…¹ìŒ | âœ… ì™„ë£Œ | MediaRecorder APIë¡œ ìŒì„± ë…¹ìŒ |
| STT ë³€í™˜ | âœ… ì™„ë£Œ | OpenAI Whisperë¡œ ìŒì„±â†’í…ìŠ¤íŠ¸ |
| TTS ìƒì„± | âœ… ì™„ë£Œ | OpenAI TTSë¡œ í…ìŠ¤íŠ¸â†’ìŒì„± |
| ìë™ ì¬ìƒ | âœ… ì™„ë£Œ | ë´‡ ì‘ë‹µ í›„ ìë™ ìŒì„± ì¬ìƒ |
| ìˆ˜ë™ ì¬ìƒ | âœ… ì™„ë£Œ | "ìŒì„± ë“£ê¸°" ë²„íŠ¼ìœ¼ë¡œ ì¬ìƒ |
| ì¬ìƒ ì œì–´ | âœ… ì™„ë£Œ | ì¬ìƒ ì¤‘ ì¤‘ì§€ ê°€ëŠ¥ |
| ê¶Œí•œ ê´€ë¦¬ | âœ… ì™„ë£Œ | ë´‡ë³„ enableVoiceInput/Output ì„¤ì • |
| UI/UX | âœ… ì™„ë£Œ | ë…¹ìŒ ì¤‘ ì• ë‹ˆë©”ì´ì…˜, ì¬ìƒ ìƒíƒœ í‘œì‹œ |

## ğŸ”§ ìˆ˜ì •ëœ íŒŒì¼

- âœ… `src/app/dashboard/ai-gems/[gemId]/page.tsx` - ìŒì„± ê¸°ëŠ¥ í´ë¼ì´ì–¸íŠ¸ ë¡œì§
- âœ… `src/app/api/speech/transcribe/route.ts` - STT API (ì´ë¯¸ ì¡´ì¬)
- âœ… `src/app/api/speech/synthesize/route.ts` - TTS API (ì´ë¯¸ ì¡´ì¬)
- âœ… `prisma/schema.prisma` - enableVoiceInput, enableVoiceOutput í•„ë“œ (ì´ë¯¸ ì¶”ê°€ë¨)

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

### í–¥í›„ ê°œì„  ê°€ëŠ¥ ì‚¬í•­
1. **ìŒì„± ì„ íƒ**: TTS ìŒì„± ì„ íƒ ì˜µì…˜ (nova, alloy, echo ë“±)
2. **ì¬ìƒ ì†ë„**: 0.5x ~ 2.0x ì¬ìƒ ì†ë„ ì¡°ì ˆ
3. **ì–¸ì–´ ì„ íƒ**: í•œêµ­ì–´, ì˜ì–´, ì¼ë³¸ì–´ ë“± ë‹¤êµ­ì–´ ì§€ì›
4. **ìŒì„± ì €ì¥**: ìƒì„±ëœ ìŒì„± íŒŒì¼ ë‹¤ìš´ë¡œë“œ ê¸°ëŠ¥
5. **ìŒì„± íˆìŠ¤í† ë¦¬**: ì´ì „ ìŒì„± ì¬ìƒ ê¸°ë¡ í‘œì‹œ

## âš ï¸ ì£¼ì˜ì‚¬í•­

### OpenAI API ë¹„ìš©
- **Whisper (STT)**: $0.006 / ë¶„
- **TTS**: $0.015 / 1000ì (tts-1), $0.030 / 1000ì (tts-1-hd)
- ì˜ˆìƒ ë¹„ìš©: í‰ê·  ëŒ€í™” 10ë¶„ + ì‘ë‹µ 500ì â†’ ì•½ $0.067

### ë¸Œë¼ìš°ì € ì§€ì›
- âœ… Chrome/Edge: ì™„ì „ ì§€ì›
- âœ… Firefox: ì™„ì „ ì§€ì›
- âœ… Safari: ì™„ì „ ì§€ì› (ë§ˆì´í¬ ê¶Œí•œ í•„ìš”)
- âŒ IE: ë¯¸ì§€ì› (MediaRecorder API ì—†ìŒ)

### ë³´ì•ˆ
- ìŒì„± ë…¹ìŒì€ **HTTPS í•„ìˆ˜** (Vercel ìë™ ì œê³µ)
- ë§ˆì´í¬ ê¶Œí•œì€ **ì‚¬ìš©ì í—ˆìš© í•„ìš”**
- API í‚¤ëŠ” **ì„œë²„ ì¸¡ì—ì„œë§Œ ì‚¬ìš©** (í´ë¼ì´ì–¸íŠ¸ ë…¸ì¶œ ë°©ì§€)

## ğŸ“ ë¡œê·¸ ì˜ˆì‹œ

### ì„±ê³µì ì¸ ìŒì„± ì…ë ¥
```
ğŸ¤ ë…¹ìŒ ì‹œì‘...
âœ… ë…¹ìŒ ì™„ë£Œ: 3.2ì´ˆ
ğŸ“ ìŒì„± ë³€í™˜ ì¤‘...
âœ… ë³€í™˜ ì™„ë£Œ: "ì•ˆë…•í•˜ì„¸ìš”, ìˆ˜í•™ ë¬¸ì œ í’€ì´ë¥¼ ë„ì™€ì£¼ì„¸ìš”."
```

### ì„±ê³µì ì¸ ìŒì„± ì¶œë ¥
```
ğŸ”Š ìŒì„± ìƒì„± ì¤‘...
âœ… ìŒì„± ìƒì„± ì™„ë£Œ: 156ì â†’ 5.2ì´ˆ ì˜¤ë””ì˜¤
â–¶ï¸ ìŒì„± ì¬ìƒ ì‹œì‘...
âœ… ì¬ìƒ ì™„ë£Œ
```

---

## âœ… ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [x] ìŒì„± ë…¹ìŒ ê¸°ëŠ¥ êµ¬í˜„
- [x] STT API ì—”ë“œí¬ì¸íŠ¸ êµ¬í˜„
- [x] TTS API ì—”ë“œí¬ì¸íŠ¸ êµ¬í˜„
- [x] ìë™ ìŒì„± ì¬ìƒ êµ¬í˜„
- [x] ìˆ˜ë™ ìŒì„± ì¬ìƒ êµ¬í˜„
- [x] ì¬ìƒ ì¤‘ì§€ ê¸°ëŠ¥ êµ¬í˜„
- [x] UI/UX êµ¬í˜„ (ë…¹ìŒ ë²„íŠ¼, ìŒì„± ë“£ê¸° ë²„íŠ¼)
- [x] ë´‡ë³„ ìŒì„± ê¸°ëŠ¥ ì„¤ì • (enableVoiceInput, enableVoiceOutput)
- [x] ì—ëŸ¬ ì²˜ë¦¬ ë° ì‚¬ìš©ì í”¼ë“œë°±
- [ ] **Vercel í™˜ê²½ ë³€ìˆ˜ ì„¤ì •** (`OPENAI_API_KEY`) â† ğŸš¨ **ì‚¬ìš©ìê°€ ì§ì ‘ ì„¤ì • í•„ìš”!**
- [ ] í”„ë¡œë•ì…˜ ë°°í¬ í›„ í…ŒìŠ¤íŠ¸

---

**ì‘ì„±ì**: Claude AI  
**ìµœì¢… ìˆ˜ì •**: 2026-01-24  
**ë²„ì „**: 1.0  
**ê´€ë ¨ ë¬¸ì„œ**: `AI_BOT_MULTIMODAL_SYSTEM.md`, `EMOJI_PICKER_FEATURE.md`
